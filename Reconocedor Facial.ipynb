{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocedor de puntos clave faciales con Python\n",
    "## Ejemplo de implementación utilizando OpenCV y Redes Neuronales Convolucionales\n",
    "\n",
    "Aquí puedes encontrar la presentación en la que se basa esta implementación.\n",
    "\n",
    "Para hacerlo más fácil, se ha separado en diferentes pasos diferenciados por funcionalidad. Las partes son:\n",
    "\n",
    "[**Paso 1**](#step0) : Detectar las caras a partir de modelos definidos en OpenCV.\n",
    "\n",
    "[**Paso 2**](#step1) : Definición y entrenamiento de una Red Neuronal Convolucional (CNN) en Keras.\n",
    "\n",
    "[**Paso 3**](#step1) : Aplicar el conocimiento de la CNN a cada una de las caras detectadas en las imágenes en el [**Paso 1**](#step0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for this section\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cv2                     # OpenCV library for computer vision\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "# Auxiliary functions\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\" Method to read an image from file to matrix \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def plot_image(image, title=''):\n",
    "    \"\"\" It plots an image as it is in a single column plot \"\"\"\n",
    "    # Plot our image using subplots to specify a size and title\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    ax1.imshow(image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step0'></a>\n",
    "# Paso 1: Detectar las caras a partir de modelos definidos en OpenCV\n",
    "\n",
    "Definición de los métodos que utilizaremos.\n",
    "\n",
    "get_faces: A partir de una imagen se devuelve el conjunto de caras detectadas\n",
    "\n",
    "plot_faces: Muestra las caras detectadas sobre la imagen original\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_faces(image):\n",
    "    \"\"\"\n",
    "    It returns an array with the detected faces in an image\n",
    "    Every face is defined as OpenCV does: top-left x, top-left y, width and height.\n",
    "    \"\"\"\n",
    "    # To avoid overwriting\n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    # The filter works with grayscale images\n",
    "    gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Extract the pre-trained face detector from an xml file\n",
    "    face_classifier = cv2.CascadeClassifier('detectors/haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detect the faces in image\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    return faces \n",
    "\n",
    "def draw_faces(image, faces=None, plot=True):\n",
    "    \"\"\"\n",
    "    It plots an image with its detected faces. If faces is None, it calculates the faces too\n",
    "    \"\"\"\n",
    "    if faces is None:\n",
    "        faces = get_faces(image)\n",
    "    \n",
    "    # To avoid overwriting\n",
    "    image_with_faces = np.copy(image)\n",
    "    \n",
    "    # Get the bounding box for each detected face\n",
    "    for (x,y,w,h) in faces:\n",
    "        # Add a red bounding box to the detections image\n",
    "        cv2.rectangle(image_with_faces, (x,y), (x+w,y+h), (255,0,0), 3)\n",
    "        \n",
    "    if plot is True:\n",
    "        plot_image(image_with_faces)\n",
    "    else:\n",
    "        return image_with_faces\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_image_with_keypoints(image, image_info):\n",
    "    \"\"\"\n",
    "    It plots keypoints given in (x,y) format\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    \n",
    "    for (face, keypoints) in image_info:\n",
    "        for (x,y) in keypoints:\n",
    "            ax1.scatter(x, y, marker='o', c='c', s=10)\n",
    "   \n",
    "\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.imshow(image)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = read_image('images/breaking_bad.jpg')\n",
    "faces = get_faces(image)\n",
    "print(\"Faces detected: {}\".format(len(faces)))\n",
    "draw_faces(image, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "# Paso 2:  Definición y entrenamiento de una Red Neuronal Convolucional (CNN) en Keras\n",
    "\n",
    "En este paso se define una red neuronal convolucional para sacar los keypoints de cada una de las fotos detectadas. La implementación se realizará en Keras, usando Tensorflow como backend.\n",
    "\n",
    "Para poder hacer uso del conocimiento de la red neuronal hay que entrenarla con datos previos. En kaggle podemos encontrar [este dataset](https://www.kaggle.com/c/facial-keypoints-detection/data) que contiene 60MB  de imágenes, cada una con un tag en los keypoints.\n",
    "\n",
    "Estos keypoints serán 15: 4 en la boca, 1 en la nariz, 3 en cada ojo y 2 en cada ceja.\n",
    "\n",
    "Lo primero que haremos será cargar estas imágenes y prepararlas para el entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "# Load training set\n",
    "# load_data es un método definido en el fichero utils para cargar las imágenes.\n",
    "X_train, y_train = load_data()\n",
    "print(\"X_train.shape == {}\".format(X_train.shape))\n",
    "print(\"y_train.shape == {}; y_train.min == {:.3f}; y_train.max == {:.3f}\".format(\n",
    "    y_train.shape, y_train.min(), y_train.max()))\n",
    "\n",
    "# Load testing set\n",
    "X_test, _ = load_data(test=True)\n",
    "print(\"X_test.shape == {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de los datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    plot_data(X_train[i], y_train[i], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la Red Neuronal Convolucional\n",
    "\n",
    "La red neuronal definida tiene una complejidad básica-media. No es el objetivo entenderla al 100%. Puede ser interesante ver cómo se alternan capas convolucionales que dan profundidad a la red con capas de \"pooling\", que reducen el tamaño de las imágenes.\n",
    "\n",
    "También se puede ver cómo se han añadido capas de Dropout (descarte de un porcentaje de los resultados intermedios) para evitar el overfitting.\n",
    "\n",
    "Al final se termina con una capa densa de 30 valores, un valor por cada posición \"x\" e \"y\" de los 15 keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import deep learning resources from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "\n",
    "# Your model should accept 96x96 pixel graysale images in\n",
    "# It should have a fully-connected output layer with 30 values (2 for each facial keypoint)\n",
    "shape = (96,96)\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(16,(2,2),padding='same',input_shape=(96,96, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(64,(3,3),padding='same',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(128,(3,3),padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(30))\n",
    "\n",
    "\n",
    "# Summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilación y entrenamiento de la Red Neuronal Convolucional\n",
    "\n",
    "Para compilar y entrenar una red neuronal hay un millón de combinaciones posibles dependiendo de la configuración de lo que se denominan \"hiperparámetros\".\n",
    "\n",
    "Tampoco es objeto de estudio la optimización de una red neuronal, por lo que se han realizado diferentes pruebas y los mejores resultados obtenidos han sido los siguientes (¡te animamos a que pruebes los tuyos!):\n",
    "\n",
    " optimizer: adam\n",
    "\n",
    "loss: mse\n",
    "\n",
    "batch_size: 20\n",
    "\n",
    "validation:split: 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.callbacks import ModelCheckpoint, History  \n",
    "\n",
    "epochs = 50\n",
    "histo = History()\n",
    "\n",
    "## Compile the model\n",
    "def compile_model(model, epochs):\n",
    "    \n",
    "    filepath = 'model.hdf5'\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath=filepath, \n",
    "                                   verbose=1, save_best_only=True)\n",
    "\n",
    "    ## Train the model\n",
    "    hist = model.fit(X_train, y_train, validation_split=0.2,\n",
    "              epochs=epochs, batch_size=20, callbacks=[checkpointer, histo], verbose=1)\n",
    "    \n",
    "    model.save(filepath)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "def show_training_validation_loss(hist, epochs):\n",
    "    plt.plot(range(epochs), hist.history[\n",
    "             'val_loss'], 'g-', label='Val Loss')\n",
    "    plt.plot(range(epochs), hist.history[\n",
    "             'loss'], 'g--', label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "# TODO: Set True if you want to train the network. It will get a pretrained network values from a file.\n",
    "train_net = False\n",
    "\n",
    "if train_net is True:\n",
    "    hist = compile_model(model, epochs) \n",
    "else:\n",
    "    model.load_weights('model.hdf5')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the training and validation loss of the neural network\n",
    "if train_net is True:\n",
    "    show_training_validation_loss(hist, epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of an almost ideal training process: \n",
    "\n",
    "<img src=\"images/adam.png\" width=400 height=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='step2'></a>\n",
    "# Paso 3:  Aplicar el conocimiento de la CNN a cada una de las caras detectadas\n",
    "\n",
    "Vamos a elegir la misma imagen que al al principio, y vamos a representar en las caras los keypoints de cada uno.\n",
    "\n",
    "La red neuronal se ha entrenado con imágenes de 96x96, por lo que habrá que escalar el rectángulo que delimita las caras a esa escala para poder extraer los keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints(image, faces=None):\n",
    "    \n",
    "    # list of pairs (face, keypoints)\n",
    "    result = []\n",
    "    \n",
    "    if faces is None:\n",
    "        faces = get_faces(image)\n",
    "    \n",
    "    # Same size than training/validation set\n",
    "    faces_shape = (96, 96)\n",
    "    \n",
    "    # To avoid overwriting\n",
    "    image_copy = np.copy(image)\n",
    "    \n",
    "    # For each face, we detect keypoints and show features\n",
    "    for (x,y,w,h) in faces:\n",
    "\n",
    "        # We crop the face region\n",
    "        face = image_copy[y:y+h,x:x+w]\n",
    "\n",
    "        # Face converted to grayscale and resize (our CNN receives images of 96x96x1)\n",
    "        gray_face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "        resize_gray_face = cv2.resize(gray_face, faces_shape) / 255\n",
    "\n",
    "        # Formatting x inputs. Inputs will have format of (1, 96, 96, 1)\n",
    "        inputs = np.expand_dims(np.expand_dims(resize_gray_face, axis=-1), axis=0)\n",
    "                                \n",
    "        # Get keypoints result                        \n",
    "        predicted_keypoints = model.predict(inputs)\n",
    "\n",
    "        # All keypoints in a single flat array. We will retrieve keypoints as (x,y) with (idx, idx+1) values.\n",
    "        predicted_keypoints = np.squeeze(predicted_keypoints)\n",
    "        \n",
    "        keypoints = []        \n",
    "        for idx in range(0, len(predicted_keypoints), 2):\n",
    "            # Scale factor (revert scale)\n",
    "            x_scale_factor = face.shape[0]/faces_shape[0] \n",
    "            y_scale_factor = face.shape[1]/faces_shape[1] \n",
    "\n",
    "            # Offset of the center of the scatter\n",
    "            x_center_left_offset = predicted_keypoints[idx] * faces_shape[0]/2 + faces_shape[0]/2 \n",
    "            y_center_left_offset = predicted_keypoints[idx + 1] * faces_shape[1]/2 + faces_shape[1]/2\n",
    "            \n",
    "            x_center = int(x + (x_scale_factor * x_center_left_offset))\n",
    "            y_center = int(y + (y_scale_factor * y_center_left_offset))\n",
    "\n",
    "            keypoints.append([x_center, y_center])\n",
    "        \n",
    "        result.append([(x,y,w,h), keypoints])\n",
    "    \n",
    "    return result\n",
    "\n",
    "def show_image_and_features(image_path):\n",
    "    image = read_image(image_path)\n",
    "    faces = get_faces(image)\n",
    "    keypoints = get_keypoints(image, faces)\n",
    "    image_with_faces = draw_faces(image, faces ,plot=False)\n",
    "    plot_image_with_keypoints(image_with_faces, keypoints)\n",
    "\n",
    "show_image_and_features('images/breaking_bad.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a probar el algoritmo en una image mayor para observar las características más en detalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_and_features('images/saul_goodman.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra. ¿Nos atrevemos a ponerle una gafas a partir de los keypoints?\n",
    "\n",
    "Lo primero que tenemos que saber qué índice contiene qué keypoint. En la documentacion del dataset de kaggle podemos encontrar:\n",
    "\n",
    "left_eye_center, right_eye_center, left_eye_inner_corner, left_eye_outer_corner, right_eye_inner_corner, right_eye_outer_corner, left_eyebrow_inner_end, left_eyebrow_outer_end, right_eyebrow_inner_end, right_eyebrow_outer_end, nose_tip, mouth_left_corner, mouth_right_corner, mouth_center_top_lip, mouth_center_bottom_lip\n",
    "\n",
    "Cuando hacemos referencia a izquierda o derecha es con respecto a la persona. En la imagen anterior, el ojo izquierdo es el que tiene un valor de x mayor.\n",
    "\n",
    "Después lo adaptamos a nuestra imagen.\n",
    "\n",
    "<img src=\"images/thug_life_with_range.png\" width=400 height=300/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we show the (guessed) bounding rectangle for the eyes underneath the sunglasses\n",
    "# We'll use the bounding rectangle to figure out how to map the glasses on the eye keypoints\n",
    "# Values were obtained via experimentation\n",
    "# glasses_triangle_vertices = np.array([(280,220), (2800,220), (280,600)]).astype(np.float32)\n",
    "thug_glasses_triangle_vertices = np.array([(65,10), (490,10), (65,70)]).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def thug_image(image):\n",
    "    faces = get_faces(image)\n",
    "    image_info = get_keypoints(image, faces)\n",
    "    \n",
    "    # Load in sunglasses image - note the usage of the special option\n",
    "    # cv2.IMREAD_UNCHANGED, this option is used because the sunglasses \n",
    "    # image has a 4th channel that allows us to control how transparent each pixel in the image is\n",
    "    sunglasses = cv2.imread(\"images/thug_life.png\", cv2.IMREAD_UNCHANGED)    \n",
    "    alpha_channel = sunglasses[:,:,3]\n",
    "    \n",
    "    for (face, keypoints) in image_info:\n",
    "    \n",
    "        # We keep only the keypoints related to eyes (from 0 to 9)\n",
    "        eye_keypoints = keypoints[:10]\n",
    "\n",
    "        # Compute the bounding rectangle for the eyes\n",
    "        eye_boundingRect = cv2.boundingRect(np.array(eye_keypoints).astype(np.float32))\n",
    "        # Build the triangle vertices needed by cv2.getAffineTransform()\n",
    "        eyes_triangle_vertices = np.array([(eye_boundingRect[0],eye_boundingRect[1]), \n",
    "                                           (eye_boundingRect[0]+eye_boundingRect[2],eye_boundingRect[1]), \n",
    "                                           (eye_boundingRect[0],eye_boundingRect[1]+eye_boundingRect[3])]).astype(np.float32)\n",
    "        # Compute the affine transform matrix from the two sets of three points (glasses and eyes)\n",
    "        map_matrix = cv2.getAffineTransform(thug_glasses_triangle_vertices, eyes_triangle_vertices)\n",
    "        # Apply the affine transformation to the glasses\n",
    "        transformed_sunglasses = cv2.warpAffine(sunglasses, map_matrix, (image.shape[1], image.shape[0]))\n",
    "        # Build a binary mask of the pixels where the sunglasses are\n",
    "        transformed_sunglasses_mask = transformed_sunglasses[:,:,3] > 0\n",
    "        # Overwrite pixels in the original image with sunglasses pixels using their mask\n",
    "        image[:,:,:][transformed_sunglasses_mask] = transformed_sunglasses[:,:,0:3][transformed_sunglasses_mask]\n",
    "    \n",
    "    return image   \n",
    "\n",
    "thug_life_image = thug_image(read_image('images/saul_goodman.jpg'))\n",
    "plot_image(thug_life_image)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thug_life_image = thug_image(read_image('images/breaking_bad.jpg'))\n",
    "plot_image(thug_life_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra 2. Aplica el filtro con la web cam!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time \n",
    "from keras.models import load_model\n",
    "def laptop_camera_go():\n",
    "    # Create instance of video capturer\n",
    "    cv2.namedWindow(\"face detection activated\")\n",
    "    vc = cv2.VideoCapture(0)\n",
    "\n",
    "    # Try to get the first frame\n",
    "    if vc.isOpened(): \n",
    "        rval, frame = vc.read()\n",
    "    else:\n",
    "        rval = False\n",
    "    \n",
    "    # keep video stream open\n",
    "    while rval:\n",
    "        # plot image from camera with detections marked\n",
    "        frame = thug_image(frame)\n",
    "        cv2.imshow(\"face detection activated\", frame)\n",
    "        \n",
    "        # exit functionality - press any key to exit laptop video\n",
    "        key = cv2.waitKey(20)\n",
    "        if key > 0: # exit by pressing any key\n",
    "            # destroy windows\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            # hack from stack overflow for making sure window closes on osx --> https://stackoverflow.com/questions/6116564/destroywindow-does-not-close-window-on-mac-using-python-and-opencv\n",
    "            for i in range (1,5):\n",
    "                cv2.waitKey(1)\n",
    "            return\n",
    "        \n",
    "        # read next frame\n",
    "        time.sleep(0.05)             # control framerate for computation - default 20 frames per sec\n",
    "        rval, frame = vc.read()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run your keypoint face painter\n",
    "laptop_camera_go()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aind-cv",
   "language": "python",
   "name": "aind-cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
